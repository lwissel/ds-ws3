Data Structures End-Semester Assignment

Student:    Lennart Wissel (ICY)
Number:     1502856


1) Quicksort

a) Comparison of three strategies for choosing a pivot, portionsize >= 10.

i) Rightmost entry
   This is more or less equal to the simple 'take the first element' rule.
   Assuming a statistical chaotic distribution of keys the average performance
   will be quite fine. In practice however, you often face ordered or partially
   ordered (or reverse ordered) portions where choosing the right (left)-most
   key would end up in maximum performance setback. The algorithm would only
   partition the chosen element from the rest of the portion ending up with
   worst-case O(n^2) behaviour.

ii) Median of three (left, mid, right)
   The median of three elements {a, b, c} is defined as b, after ordering the
   elements a <= b <= c. This means we have to add three more compare
   operations and six assignments in worst-case to check which of the picked
   elements is 'b'. Pseudocode (3! = 6 permutations are possible):
   For any {a, b, c}
      if (a > b) swap(a,b)
      if (b > c) swap(b,c)
      if (a > b) swap(a,b)
   where swap(x,y) swaps the values of x and y. Then the pivot is at b.
   
   The advantage is that we simply avoid case (i), because the more ordered a
   portion is the more the median of first-middle-last equals the median of the
   whole portion.

   Even though this method avoids the problem with semi-ordered portions as
   described in (i), one could also think about choosing three random elements
   rather than first-middle-last. There is always a possiblity to choose pivots
   at the end of the portion. Also, we have to consider the performance of the
   comparisons for the given element type to find the median, which might be
   costly for complex keys.

iii) Median of all the entries
   This looks like the best idea on first sight, because it would lead to
   partitions of (almost) equal size. The only inevitable problem is that we
   need to order the portion first to find the median, which is therefore not
   feasible.


b) Quicksort the given array with the median of left-(left)middle-right pivot.
   - (pivot) around the pivot before partitioning
   - [pivot] around the pivot after partitioning
   - values >= pivot are placed after the pivot

   Given:   3, 17, 1, 5, 4, 20, 16, 3, 12, 7, 5, 31, 8, 10, 2 

   Step 1:
      Pivot out of {3,3,2} is 3.
            3, 17, 1, 5, 4, 20, 16, (3), 12, 7, 5, 31, 8, 10, 2
      After partitioning:
            1, 2, [3], 3, 17, 5, 4, 20, 16, 12, 7, 5, 31, 8, 10 

   Step 2:
      Pivot left subarray: Size == 2, just compare and swap if needed.
      Left subarray is sorted.
      Pivot right subarray: Out of {3,16,10} is 10.
            3, 17, 5, 4, 20, 16, 12, 7, 5, 31, 8, (10)
      After partitioning:
            3, 5, 4, 7, 5, 8, [10], 17, 20, 16, 12, 31

   Step 3:
      Pivot left subarray: Out of {3,4,8} is 4.
      Left subarray: 3, 5, (4), 7, 5, 8
      Left subarray after partitioning: 3, [4], 5, 7, 5, 8

      Pivot right subarray: Out of {17,16,31} is 17.
      Right subarray: (17), 20, 16, 12, 31
      Right subarray after partitioning: 16, 12, [17], 20, 31

      All remaining portions are now of size <= 4, therefore we can sort in one
      step and receive the final sorted array:
         1, 2, 3, 3, 4, 5, 5, 7, 8, 10, 12, 16, 17, 20, 31


2) Hash tables

a) Inserting and deleting values x into a hash table of size T = 11 using
   double hashing. Let dn(x) yield digit n of x. Then:

   Primary hash function   p(x) = (d2(x) + d4(x)) % 11
   Secondary hash function s(x) = (d3(x) % 5) + 1

   The secondary hash function should be used to skip leftwards, so the
   sequence of hash functions is h(x,i) = (p(x) - i*s(x)) % T, with
   incrementing i (natural number) until an empty slot is found. The mod T at
   the end ensures to stay within the array bounds in circular fashion.
   t marks a tombstone. I assume 'skipping left' means to use the secondary
   hash to advance with left-circular steps.

   Hash table:
   Slots        0  1  2  3  4  5  6  7  8  9  10
               |  |  |  |  |  |  |  |  |  |  |  |

   Step 1: Insert 25732, p(x) = 8, s(x) = 3
    0  1  2  3  4  5  6  7  8       9  10
   |  |  |  |  |  |  |  |  | 25732 |  |  |

   Step 2: Insert 41850, p(x) = 6, s(x) = 4
    0  1  2  3  4  5  6       7  8       9  10
   |  |  |  |  |  |  | 41850 |  | 25732 |  |  |

   Step 3: Insert 27119, p(x) = 8, s(x) = 2
    0  1  2  3  4       5  6       7  8       9  10
   |  |  |  |  | 27119 |  | 41850 |  | 25732 |  |  |

   Step 4: Delete 41850, p(x) = 6, s(x) = 4
    0  1  2  3  4       5  6   7  8       9  10
   |  |  |  |  | 27119 |  | t |  | 25732 |  |  |

   Step 5: Insert 39721, p(x) = 0, s(x) = 3
    0       1  2  3  4       5  6   7  8       9  10
   | 39721 |  |  |  | 27119 |  | t |  | 25732 |  |  |

   Step 6: Insert 46211, p(x) = 7, s(x) = 3
    0       1  2  3  4       5  6   7       8        9  10
   | 39721 |  |  |  | 27119 |  | t | 46211 | 25732  |  |  |

   Step 7: Insert 32054, p(x) = 7, s(x) = 1
    0       1  2  3  4       5  6       7       8        9  10
   | 39721 |  |  |  | 27119 |  | 32054 | 46211 | 25732  |  |  |

   Step 8: Delete 25732, p(x) = 8, s(x) = 3
    0       1  2  3  4       5  6       7       8   9  10
   | 39721 |  |  |  | 27119 |  | 32054 | 46211 | t |  |  |

   Step 9: Insert 43416, p(x) = 4, s(x) = 5
    0       1  2  3  4       5  6       7       8   9  10
   | 39721 |  |  |  | 27119 |  | 32054 | 46211 | t |  | 43416 |

   Step 9 represents the final hash table after the given instructions.

b) Should the load factor of a hash table also account for tombstones?
   
   The answer to this question is heavily depending on what a 'load factor' of
   a hash table should be able to represent. To elaborate this a bit further I
   will have a look at the terms occuring in the context of this question
   first:
   
   To my knowledge the load factor 'a' of a hash table is defined as a = n/T,
   with n being the current number of entries, and T being the size or
   capacity of the table. Depending on the hashing method the load factor is
   an important value to determine how efficient probing algorithms run or
   whether they might not even terminate at all. It can also indicate when a
   hash table size should be increased to ensure continuously good
   performance.

   Moreover, the use of tombstones serves at least two reasons. Deleting a
   record must not stop any search performed, so that entries further down in
   the chain are still found. The tombstone indicates that the look-up
   algorithm is not allowed to terminate before having followed the process
   chain to the end. This also applies for insertions to avoid duplicate
   entries. On the other hand it ensures that the free slot can be used again
   by another key.

   That being said, the tombstones are not included in the load factor by
   definition, but they indicate where performance could be enhanced by
   reducing probing chains which are lengthened after mixed insertions and
   deletions. With a load factor and its purpose to only indicate how full a
   table is there is no need to include the tombstones in it, as a tombstone
   represents a free slot.

   I would suggest to handle the amount and distribution of tombstones
   independently instead, as to implement some kind of internal reorganization
   of tombstones (eg swapping them to the end of the chain) or hash table
   rebuilding strategies when they reach an upper boundary that is likely to
   cause performance downgrade. This could be summed up in another factor, so
   that the original meaning of the load factor stays untouched.
